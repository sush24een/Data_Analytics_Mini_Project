# -*- coding: utf-8 -*-
"""CQoLR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AXyDBPMWFj8CR0TwVJoJ4m2lu83KMch

**IMPORTING NECESSARY LIBRARIES, LOADING DATASET**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

"""Dataset obtained from [Kaggle](https://www.kaggle.com/orhankaramancode/city-quality-of-life-dataset)

Explanation of columns from [Teleport](https://teleport.org/cities/)
"""

data_city=pd.read_csv('/content/uaScoresDataFrame.csv')
#Storing the original dataset in another dataframe before doing modifications
og_data_city = data_city

#Checking if dataset was loaded correctly, displaying top 5 records
data_city.info()
data_city.head()

"""**PREPROCESSING**"""

#Dropping the 'Unnamed: 0' column for convenience
data_city = data_city.drop(columns = ['Unnamed: 0'],axis=1)
data_city

#Retrieving statistical information about all columns
data_city.describe()

#Retrieving all continents
continent_list = pd.unique(data_city['UA_Continent'])
print("Number of continents : ", len(continent_list))
continent_list

#Retrieving all countries
country_list = pd.unique(data_city['UA_Country'])
print("Number of countries : ", len(country_list))
country_list

#Removing outliers
cols = ['Housing','Healthcare','Environmental Quality','Cost of Living', 'Business Freedom','Safety','Education','Economy','Taxation','Startups', 'Venture Capital','Travel Connectivity','Commute','Internet Access', 'Leisure & Culture','Tolerance','Outdoors']
q1 = data_city[cols].quantile(0.25)
q3 = data_city[cols].quantile(0.75)
#Interquartile range
iqr = q3-q1 
fence_low  = q1-1.5*iqr
fence_high = q3+1.5*iqr
w_outliers = data_city
data_city = data_city[~((data_city[cols] < (q1 - 1.5 * iqr)) |(data_city[cols] > (q3 + 1.5 * iqr))). any(axis=1)]

print("GRAPHS BEFORE AND AFTER REMOVING OUTLIERS")
w_outliers.plot()
data_city.plot()

"""The difference in the graphs between the original dataset and the one after outliers have been removed signifies that outliers comprise a large section of our data, and this cleaning is necessary to obtain the correct results in further steps."""

#Normalising data using MinMax scaling method. 
data_city['Housing'] = MinMaxScaler().fit_transform(np.array(data_city['Housing']).reshape(-1,1))
data_city['Cost of Living'] = MinMaxScaler().fit_transform(np.array(data_city['Cost of Living']).reshape(-1,1))
data_city['Startups'] = MinMaxScaler().fit_transform(np.array(data_city['Startups']).reshape(-1,1))
data_city['Venture Capital'] = MinMaxScaler().fit_transform(np.array(data_city['Venture Capital']).reshape(-1,1))
data_city['Travel Connectivity'] = MinMaxScaler().fit_transform(np.array(data_city['Travel Connectivity']).reshape(-1,1))
data_city['Commute'] = MinMaxScaler().fit_transform(np.array(data_city['Commute']).reshape(-1,1))
data_city['Business Freedom'] = MinMaxScaler().fit_transform(np.array(data_city['Business Freedom']).reshape(-1,1))
data_city['Safety'] = MinMaxScaler().fit_transform(np.array(data_city['Safety']).reshape(-1,1))
data_city['Healthcare'] = MinMaxScaler().fit_transform(np.array(data_city['Healthcare']).reshape(-1,1))
data_city['Education'] = MinMaxScaler().fit_transform(np.array(data_city['Education']).reshape(-1,1))
data_city['Environmental Quality'] = MinMaxScaler().fit_transform(np.array(data_city['Environmental Quality']).reshape(-1,1))
data_city['Economy'] = MinMaxScaler().fit_transform(np.array(data_city['Economy']).reshape(-1,1))
data_city['Taxation'] = MinMaxScaler().fit_transform(np.array(data_city['Taxation']).reshape(-1,1))
data_city['Internet Access'] = MinMaxScaler().fit_transform(np.array(data_city['Internet Access']).reshape(-1,1))
data_city['Leisure & Culture'] = MinMaxScaler().fit_transform(np.array(data_city['Leisure & Culture']).reshape(-1,1))
data_city['Tolerance'] = MinMaxScaler().fit_transform(np.array(data_city['Tolerance']).reshape(-1,1))
data_city['Outdoors'] = MinMaxScaler().fit_transform(np.array(data_city['Outdoors']).reshape(-1,1))
data_city.head()

"""We have chosen this scaler by the process of elimination - 
1. we have removed outliers
2. we do not require our data to be centred around 0
3. we have no negative values

These facts remove the necessity for using the Standard scaler or the Robust scaler.

**EDA AND VISUALISATION**
"""

#Correlation between numeric fields and correlation heat map
data_city.corr()
f,ax = plt.subplots(figsize=(15,15))
sns.heatmap(data_city.corr(), annot=True, linewidths=0.5,linecolor="cyan", fmt= '.3f',ax=ax)
plt.show()

"""The following notable insights can be gained from the heat map above -

1. Internet Access and Education show a high positive correlation. They are directly proportional.
2. Education and Environmental Quality show a high positive correlation with Business Freedom.
3. Economy is highly negatively correlated with both Cost of Living and Housing.
4. Housing and Environmental Quality are negatively correlated with each other.
"""

#Swarmplot between continents and economy
plt.figure(figsize=(10,5))
sns.swarmplot(x='UA_Continent',y='Economy',data=data_city,size=7)
plt.xticks(rotation=45)
plt.xlabel('CONTINENTS')
plt.ylabel('ECONOMY')

"""The above plot of Continents v/s Economy tells us that the economy distribution in European cities is mostly centred about the median, with the outliers being on the extremes. However, North American cities have a distribution skewed towards the upper limit, while African and South American cities are skewed towards the lower. """

plt.figure(figsize=(10,5))
sns.swarmplot(x='UA_Continent',y='Cost of Living',data=data_city,size=7)
plt.xticks(rotation=45)
plt.xlabel('CONTINENTS')
plt.ylabel('COST of LIVING')

plt.figure(figsize=(10,5))
sns.swarmplot(x='UA_Continent',y='Business Freedom',data=data_city,size=3)
plt.xticks(rotation=45)
plt.xlabel('CONTINENTS')
plt.ylabel('BUSINESS FREEDOM')

f,ax=plt.subplots(figsize=(7,5))
sns.barplot(y='UA_Continent',x='Safety',data=data_city,color='black',alpha=0.2,label='Safety')
sns.barplot(y='UA_Continent',x='Economy',data=data_city,color='blue',alpha=0.3,label='Economy')
sns.barplot(y='UA_Continent',x='Cost of Living',data=data_city,color='green',alpha=0.2,label='Cost of Living')
sns.barplot(y='UA_Continent',x='Business Freedom',data=data_city,color='red',alpha=0.5,label='Business Freedom')

plt.ylabel('Continents')
plt.xlabel('Values')
plt.xticks(rotation=0)
plt.yticks(rotation=45)
plt.title('Average Values of the Continents')
ax.legend(loc='lower right',frameon = True)

plt.show()

"""The above graph tells us that -

1. Europe has the highest average values for Business Freedom which does not vary throughout the data for European cities. Asia and Oceania have the next highest values but the former has varying values for the attribute.
2. Africa has the highest average value for the attribute Safety but most of its attribute values vary throught the data.
3. Europe, Oceania and North America have the least varying values among all the given continents.
"""

citySafety=pd.DataFrame(zip(data_city.UA_Name, data_city.Safety))
citySafety.columns=['City','Safety']
new_index=(data_city.Safety.sort_values(ascending=False)).index.values
citySafety=citySafety.reindex(new_index)

#BarPlot

#Safest
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='Safety',data=citySafety[:50],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Top Most Safest Cities')
plt.show()
#Most Unsafe
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='Safety',data=citySafety[-50:],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Top Most Unsafe Cities')
plt.show()

cityCost=pd.DataFrame(zip(data_city.UA_Name, data_city['Cost of Living']))
cityCost.columns=['City','CostLive']
new_index=(data_city['Cost of Living'].sort_values(ascending=False)).index.values
cityCost=cityCost.reindex(new_index)
cityCost.drop(cityCost[cityCost.CostLive<=0].index,inplace=True)


        
#BarPlot

#Lowest
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='CostLive',data=cityCost[-50:],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Cities with the Lowest Cost of Living')
plt.show()

#Highest
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='CostLive',data=cityCost[:50],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Cities with the Highest Cost of Living')
plt.show()

cityBusFree=pd.DataFrame(zip(data_city.UA_Name, data_city['Business Freedom']))
cityBusFree.columns=['City','Business Freedom']
new_index=(data_city['Business Freedom'].sort_values(ascending=False)).index.values
cityBusFree=cityBusFree.reindex(new_index)



        
#BarPlot

#Highest
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='Business Freedom',data=cityBusFree[:50],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Cities with the Highest Business Freedom')
plt.show()

#Lowest
plt.figure(figsize=(20,8))
sns.barplot(x='City',y='Business Freedom',data=cityBusFree[-50:],palette=sns.cubehelix_palette(50))
plt.xticks(rotation=80)
plt.title('Cities with the Lowest Business Freedom')
plt.show()

"""**MODELLING**"""

#Creating the new dataframe with weighted columns (weights assigned using Maslow's hierarchy)
data_city[['Housing','Healthcare','Environmental Quality']] = data_city[['Housing','Healthcare','Environmental Quality']] * 100
data_city[['Cost of Living', 'Business Freedom','Safety','Education','Economy','Taxation']] = data_city[['Cost of Living', 'Business Freedom','Safety','Education','Economy','Taxation']] * 75
data_city[['Startups', 'Venture Capital','Travel Connectivity','Commute']] = data_city[['Startups', 'Venture Capital','Travel Connectivity','Commute']] * 50
data_city[['Internet Access', 'Leisure & Culture','Tolerance','Outdoors']] = data_city[['Internet Access', 'Leisure & Culture','Tolerance','Outdoors']] * 25
data_city.head()

#Adding new column to store predicted values that will be used later to create ranking
data_city['QoL'] = '0'
data_city.head()

#Splitting into train and test data (80:20) - rows
train_city = data_city.loc[0:212]
test_city = data_city.loc[213:265]

#Calculating QoL column by aggregating previous numeric and weighted columns
train_city['QoL'] = train_city[['Housing','Healthcare','Environmental Quality','Cost of Living', 'Business Freedom','Safety','Education','Economy','Taxation','Startups', 'Venture Capital','Travel Connectivity','Commute','Internet Access', 'Leisure & Culture','Tolerance','Outdoors']].sum(axis=1)
print(train_city)

#Creating validation set
test_city_compare = test_city
test_city_compare['QoL'] = test_city[['Housing','Healthcare','Environmental Quality','Cost of Living', 'Business Freedom','Safety','Education','Economy','Taxation','Startups', 'Venture Capital','Travel Connectivity','Commute','Internet Access', 'Leisure & Culture','Tolerance','Outdoors']].sum(axis=1)
print(test_city_compare)

QoL_true_value=pd.DataFrame(test_city_compare,columns=['QoL'])
QoL_true_value.reset_index(inplace=True)
QoL_true_value=QoL_true_value['QoL'].round(decimals=5)
QoL_true_value

#Dividing into predictor and predictant columns
train_X = train_city.loc[:,data_city.columns != 'QoL']
train_y = train_city['QoL']

test_X = test_city.loc[:,data_city.columns != 'QoL']
test_y = test_city['QoL']

print('\n\nTraining Set')
print('\n train x= ', train_X.head())
print('\n train y= ', train_y.head())
print('\n test x= ', test_X.head())
print('\n test y= ', test_y.head())

train_X.shape, train_y.shape, test_X.shape, test_y.shape

#Dropping string-type columns for convenience in intermediate modelling steps
train_X_nostring = train_X.drop(columns = ['UA_Name','UA_Country','UA_Continent'],axis = 1)
test_X_nostring = test_X.drop(columns = ['UA_Name','UA_Country','UA_Continent'],axis = 1)

"""

**PREDICTION USING LINEAR REGRESSION**"""

#Linear Regression
lr_model = LinearRegression()
lr_model.fit(train_X_nostring,train_y)
lr_model.score(train_X_nostring,train_y)

#Predicting and evaluating the result
lr_y_predict = lr_model.predict(test_X_nostring)
lr_output = pd.DataFrame(lr_y_predict, columns = ["QoL"])
lr_output=lr_output.round(decimals=5)
print(lr_output)

#Comparing the result of the Linear Regression model with the true value of QoL of every city
lr_compare_result = QoL_true_value - lr_output['QoL']
lr_compare_result

"""The model has predicted the correct values.

**PREDICTION USING RANDOM FOREST**
"""

#Random Forest Regression
rf_model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=42)
rf_model.fit(train_X_nostring,train_y)
rf_model.score(train_X_nostring,train_y)

#Predicting and evaluating the result

rf_y_pred = rf_model.predict(test_X_nostring)

rf_output = pd.DataFrame(rf_y_pred, columns = ["QoL"])
rf_output=rf_output.round(decimals=5)
print(rf_output)

#Comparing the result of the Random Forest model with the true value of QoL of every city
rf_compare_result = QoL_true_value - rf_output['QoL']
rf_compare_result

"""We see that the difference between the values is not zero. Hence the model does not predict accurate values.

**RANKING CITIES**
"""

#merging the lr_output with the cities
frames = [train_city, test_city]
result = pd.concat(frames)
print(result)

# Sort the rows of dataframe by 'QoL' column
final_ranking = result.sort_values(by = 'QoL', ascending=False)

#Displaying final ranking of cities
display(final_ranking)

top_50 = final_ranking.head(50)
bottom_50 = final_ranking.tail(50)

print(top_50.groupby(['UA_Continent']).count())
print(bottom_50.groupby(['UA_Continent']).count())

"""**INSIGHTS FROM THE RESULT OBTAINED**

The above ranking we have obtained is an ordered list of cities in the world, sorted in descending order of the quality of life one can experience while living there. This ranking has been done after taking into consideration various factors that have been shown to influence quality of life.

We see that the top 10 cities to live in and their respective Quality of Life scores are
1. Singapore     756.403844
2. Munich        739.486250
3. Montreal      728.349694
4. Toronto       723.778022
5. Berlin        722.640525
6. Ottawa        710.480427
7. Vancouver     702.907698
8. Tokyo         700.853250
9. London        699.101777
10. Liverpool    697.653319

Rather than implying that these are the cities that are the best in all factors considered, this ranking tells us that these cities have the highest *aggregate* of factor scores, which means this is a more holistic ranking than if it were based on a single factor. 

Since we have used Maslow's hierarchy to assign weightage to the factors, we also take into consideration the different levels of need that each factor would go into, and have assigned the weights correspondingly. From this, we can infer that while Singapore might have a better environmental quality, Liverpool, or indeed other cities that are further down the list, might have a higher score in a category that has been assigned less weightage, say Outdoors.

From the results of the count() function we have used on the top 50 and bottom 50 rows, i.e. cities with highest quality of life and the cities with the lowest, we see that there are no countries from South America in the top 50 records. However, in the bottom 50, the continent with the most number of cities is North America. This tells us that most South American cities lie in the median of the spread, and that there is a wide difference in the quality of life across North American cities, as they span across the ranking.

Europe has the largest number of countries in the top 50, and the lowest in the bottom 50, and Asia, despite being the first continent on the list (Singapore) has a sizable count in the bottom 50 records.

Africa is present only in the bottom 50, not the top, while Oceanic cities are only present towards the upper side of the ranking.
"""

